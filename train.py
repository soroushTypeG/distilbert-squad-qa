"""train.py - Fine-tuning a QA model on SQuADUsage:    python train.pyRequirements:    - transformers, datasets, torch    - GPU recommended (4GB+ VRAM)After training, model is saved to ./trained_model"""import loggingimport torchfrom datasets import load_datasetfrom transformers import (    AutoTokenizer,    AutoModelForQuestionAnswering,    TrainingArguments,    Trainer,)from huggingface_hub import loginlogin()  # Add your only read token from https://huggingface.co# Setup logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Clear GPU memory if availabletorch.cuda.empty_cache()# Determine devicedevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")logger.info(f"Using device: {device}")if torch.cuda.is_available():    logger.info(f"GPU Name: {torch.cuda.get_device_name(0)}")    logger.info(f"CUDA Version: {torch.version.cuda}")# Load pre-trained model and tokenizermodel_name = "distilbert-base-uncased-distilled-squad"logger.info(f"Loading model and tokenizer: {model_name}")tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForQuestionAnswering.from_pretrained(model_name)model.to(device)# Load SQuAD dataset (only used for training)logger.info("Loading SQuAD dataset")dataset = load_dataset("squad")def preprocess_function(examples):    questions = [q.strip() for q in examples["question"]]    context = [C.strip() for C in examples["context"]]    inputs = tokenizer(        questions,        context,        truncation="only_second",        max_length=384,        stride=128,        return_overflowing_tokens=True,        return_offsets_mapping=True,        padding="max_length"    )    sample_mapping = inputs.pop("overflow_to_sample_mapping")    offset_mapping = inputs.pop("offset_mapping")    start_positions = []    end_positions = []    for i, offsets in enumerate(offset_mapping):        input_ids = inputs["input_ids"][i]        cls_index = input_ids.index(tokenizer.cls_token_id)        sequence_ids = inputs.sequence_ids(i)        sample_index = sample_mapping[i]        answers = examples["answers"][sample_index]        if len(answers["answer_start"]) == 0:            start_positions.append(cls_index)            end_positions.append(cls_index)        else:            start_char = answers["answer_start"][0]            end_char = start_char + len(answers["text"][0])            token_start_index = 0            while sequence_ids[token_start_index] != 1:                token_start_index += 1            token_end_index = len(input_ids) - 1            while sequence_ids[token_end_index] != 1:                token_end_index -= 1            if offsets[token_start_index][0] > start_char or offsets[token_end_index][1] < end_char:                start_positions.append(cls_index)                end_positions.append(cls_index)            else:                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:                    token_start_index += 1                start_positions.append(token_start_index - 1)                while token_end_index < len(offsets) and offsets[token_end_index][1] >= end_char:                    token_end_index += 1                end_positions.append(token_end_index)    inputs["start_positions"] = start_positions    inputs["end_positions"] = end_positions    return inputsdef train_model():    # device    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    logger.info(f"Using device: {device}")    if torch.cuda.is_available():        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")    # Load original pre-trained model    Model_name: str = "distilbert-base-uncased-distilled-squad"    logger.info(f"Loading tokenizer and model: {Model_name}")    global tokenizer  # so we can use it in preprocess_function    tokenizer = AutoTokenizer.from_pretrained(Model_name)    modeL = AutoModelForQuestionAnswering.from_pretrained(Model_name)    modeL.to(device)    # Load dataset    logger.info("Loading SQuAD dataset")    data_set = load_dataset("squad")    # Preprocess    logger.info("Preprocessing training data (subset: 2000 examples)")    train_dataset = data_set["train"].shuffle(seed=42).select(range(2000)).map(        preprocess_function,        batched=True,        remove_columns=data_set["train"].column_names,    )    logger.info("Preprocessing validation data (subset: 200 examples)")    eval_dataset = data_set["validation"].shuffle(seed=42).select(range(600)).map(        preprocess_function,        batched=True,        remove_columns=data_set["validation"].column_names,    )    # Training arguments    training_args = TrainingArguments(        output_dir="./results",        eval_strategy="epoch",        per_device_train_batch_size=8,        per_device_eval_batch_size=8,        num_train_epochs=2,        learning_rate=2e-5,        weight_decay=0.01,        logging_dir="./logs",        logging_steps=50,        save_strategy="epoch",        load_best_model_at_end=True,        metric_for_best_model="loss",        fp16=True,    )    # Trainer    logger.info("Creating Trainer")    trainer = Trainer(        model=modeL,        args=training_args,        train_dataset=train_dataset,        eval_dataset=eval_dataset,    )    # Start Training    logger.info("Starting fine-tuning...")    trainer.train()    # Save the fine-tuned model    logger.info("Saving model and tokenizer to ./trained_model")    modeL.save_pretrained("./trained_model")    tokenizer.save_pretrained("./trained_model")    logger.info("Training complete. Model saved.")if __name__ == "__main__":    train_model()